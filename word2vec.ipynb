{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import pyarabic.araby as araby\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import arabicstopwords.arabicstopwords as stp\n",
    "from farasa.stemmer import FarasaStemmer\n",
    "nltk.download('punkt')\n",
    "#Loading data from csv into data drame\n",
    "quoran = pd.read_csv(\"qurantexttanzil.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quoran = pd.read_csv(\"qurantexttanzil.csv\")\n",
    "soura = quoran[\"text\"]\n",
    "b=[]\n",
    "for a in soura:\n",
    "    b.append(araby.strip_diacritics(a))\n",
    "quoran['diacriticless'] = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = []\n",
    "for i in b:\n",
    "    print(wordpunct_tokenize(i))\n",
    "    tokenized.append(wordpunct_tokenize(i))\n",
    "# adding a new column in the data frame for \"tokenized\"\n",
    "quoran[\"tokenized\"] = tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['فما','إنهم','وعلى','وأولئك','ومما','إليك','بما','والذين','فيه','لا','ذلك','ولا','عليهم','غير','الذين','إياك','إن','إنك','وإياك','سواء','ولهم','معكم','كانوا','مثلهم', 'كمثل','فلما','حوله','فهم','وأنتم','كنتم','ولن','تحتها','قبل','وهم','مثلا','أنه','وأما','كثيرا','إليه','جميعا','إني','قال','قالوا','ونحن','كلها','فلما','قلنا','وكان','فتكونا','عنها','بعضكم','لبعض','فإما','معكم',\n",
    "         'وهي','أولم','كانت','سواء','هو','قال','لا','لكل','الذين','هذا','إنما','لك','يا','إذا','أن','عند','لها','الذي','لنا','في','لو','ذلك','إني','لكم','كذلك','قد','إليه','له','قالوا','ما','لهم','من','هل','وإن','مما','وهم','فيها','فيه','إذ','به','على','إن','حتى','بل','عليكم','أنا','هي','الأولى','لن','هم','حين','ليس','كل','بين','نحن','هؤلاء','ومن','إنه','بما','فإن','إنهم','ولا','عليهم','أم'\n",
    "          ,'عن','أو','إنا','أدراك','إلا','وما','إلى','ثم','تكونوا','أول','وأن','يكون','قل','كان','عليكم','منا','منكم','لعلكم','وأنا','أيها','وإنما','كانت','إنها','عني','عليها','علينا','فإنها','منا','عنده','وليس','وإنا','أيها','و','وقد','بني','وكانوا','جاء','أحيانا','قبل','ولما','معهم' \n",
    "  ]\n",
    "def stop_words_remover(word) :\n",
    "        if not stp.is_stop(word): \n",
    "           if not word in  stopwords: return  ''.join(word)\n",
    "        else: return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = FarasaStemmer()\n",
    "def stem(ayah) :\n",
    "     return stemmer.stem(ayah)\n",
    "\n",
    "x= \",\".join(quoran.diacriticless)\n",
    "x=stem(x)\n",
    "# adding a new column in the data frame for \"stem_text\"\n",
    "quoran[\"stem_text\"]=x.split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quran1 = [nltk.word_tokenize(i) for i in quoran['stem_text']]\n",
    "quoran[\"stemed_and_removed\"]= [\n",
    "    [stop_words_remover(word) for word in ayah ] for ayah in quran1]\n",
    "corpus = quoran[\"stemed_and_removed\"].apply(lambda x: [item for item in x if item != None])\n",
    "quoran[\"clean_text\"]=corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 100\n",
    "window_size = 15\n",
    "min_word = 0\n",
    "down_sampling = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "wv_model=Word2Vec(corpus,\n",
    "                  vector_size=embedding_size,\n",
    "                  window=window_size,\n",
    "                  min_count=min_word,\n",
    "                  workers=4,\n",
    "                  seed=42,\n",
    "                  epochs=30,\n",
    "                  sg=1,\n",
    "                  sample=down_sampling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantically_similar_words_wv = {words: [item[0] for item in wv_model.wv.most_similar([words], topn=5)]\n",
    "                  for words in ['الله']}\n",
    "\n",
    "for k,v in semantically_similar_words_wv.items():\n",
    "    print(k+\":\"+str(v))\n",
    "# we can improve our search by indicating positive words and negative ones\n",
    "wv_model.wv.most_similar(positive=['مريم','عيسى'], negative=['موسى'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wv_model.wv.similarity(w1='مريم', w2='عيسى'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = 'قال الله'\n",
    "s2 = ' محمد '\n",
    "#Compute cosine distance between two keys, the closer to one the smaller the angle the greater the match\n",
    "#while 0 means the verctors are at 90 deg and they are dissimilar\n",
    "distance = wv_model.wv.n_similarity(s1.split(), s2.split())\n",
    "print(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findSimilarSentence(arg):\n",
    "    output = [{\n",
    "        'sentence': '',\n",
    "        'score': 0, \n",
    "        'index': 0,\n",
    "    }]\n",
    "    for ayah in quoran['diacriticless']:\n",
    "        output.append({'sentence': ayah, 'score':  wv_model.wv.n_similarity(arg.lower().split(), ayah.lower().split()), 'index': quoran[quoran['diacriticless'] == ayah].index[0]})\n",
    "    return output\n",
    "\n",
    "temp = findSimilarSentence('إنك ميت وإنهم ميتون')\n",
    "temp.sort(key=lambda x: x['score'], reverse=True)\n",
    "temp[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
